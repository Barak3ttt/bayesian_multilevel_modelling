--
title: "Group project"
subtitle: "Group Number?"
author: 
  - "FirstName LastName"
  - "FirstName LastName"
  - "FirstName LastName"
  - "FirstName LastName"
  - "FirstName LastName"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: readable
    toc: true
    toc_depth: 4
    toc_float: true
    code_download: false
---

```{r setup, include = FALSE}
options(max.print= 120,
        width = 90,
        tibble.width = 80)
knitr::opts_chunk$set(echo= TRUE,
                      cache=FALSE,
                      prompt=FALSE,
                      tidy="styler",
                      comment=NA,
                      message=FALSE,
                      warning=TRUE)

knitr::opts_knit$set(width=90)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
set.seed(42)
```

# Checklist {-}

The submission includes the following.

- [ ] RMD document where it's clear what is the code that corresponds to each question. 
- [ ] Dataset
- [ ] html/PDF document with the following
    - [ ] Numbered questions and answers with text and all the necessary code.
    - [ ] Subtitle indicates the group number 
    - [ ] Name of all group members
    - [ ] Details of specification of the work done by group members (e.g., who found the data, who did the pre-processing, who answered which questions, etc).
    - [ ] Statement of technology. Did you use any AI tools? How?


# Group project {-}

For the project, we use the following packages:

```{r, message = FALSE}
## ...
```

## 1. Dataset Selection (0.5pt)
Select a dataset with clusters such as schools, regions, or people with multiple observations per individual. (From for example, https://www.kaggle.com/) It would be a good idea to choose a smallish dataset (not too many rows, e.g., less than 1000) or subset it so that fitting the models doesn't take too long. 

a. Describe the dataset with a couple of short sentences. What was its intended use? Are there papers that reference it? Provide information on how to obtain the dataset, including its source and any necessary preprocessing steps/feature engineering.


<!-- DESCRIBE IT BELOW -->

```{r get-data-direct, echo=TRUE, message=FALSE}
# 0) Install & load httr for HTTP requests
if (!requireNamespace("httr", quietly = TRUE)) {
  install.packages("httr", repos = "https://cran.rstudio.com/")
}
library(httr)

# 1) Kaggle credentials
user <- "barakazor"
key  <- "6bfceda688e1452d09191636ad94eeeb"

# 2) Define working and data directories
wd        <- getwd()
data_dir  <- file.path(wd, "data")
input_dir <- file.path(wd, "Input_Data")
if (!dir.exists(data_dir))  dir.create(data_dir, recursive = TRUE)
if (!dir.exists(input_dir)) dir.create(input_dir, recursive = TRUE)

# 3) Download the ZIP via Kaggle REST API
dataset_slug <- "kirtandelwadia/corporate-credit-rating-with-financial-ratios"
zip_path     <- file.path(data_dir, "credit_rating.zip")
download_url <- paste0("https://www.kaggle.com/api/v1/datasets/download/", dataset_slug)

resp <- GET(
  url             = download_url,
  authenticate(user, key),
  write_disk(zip_path, overwrite = TRUE),
  config(followlocation = TRUE)
)
stop_for_status(resp)

# 4) Unzip into data_dir
unzipped <- unzip(zip_path, exdir = data_dir)
csvs     <- unzipped[grepl("\\.csv$", unzipped)]
if (length(csvs) == 0) stop("No CSV found in the downloaded ZIP.")

# 5) Read the first CSV into R
df_raw <- read.csv(csvs[1], stringsAsFactors = FALSE)

# 6) Save to Input_Data/
write.csv(
  df_raw,
  file      = file.path(input_dir, "df_raw.csv"),
  row.names = FALSE
)
saveRDS(
  df_raw,
  file = file.path(input_dir, "df_raw.rds")
)
```


```{r preprocess-2016, echo=TRUE, message=FALSE}
# Ensure necessary packages are loaded
library(dplyr)
library(lubridate)


# Read and clean raw CSV
df_raw <- read_csv(file.path(input_dir, "df_raw.csv")) %>%
  clean_names()

# Define rating levels and create ordered factor
target_levels <- c("AAA","AA+","AA","AA-","A+","A","A-",
                   "BBB+","BBB","BBB-","BB+","BB","BB-",
                   "B+","B","B-","CCC","CC","C","D")

df <- df_raw %>%
  mutate(
    rating_ord  = factor(rating, levels = target_levels, ordered = TRUE),
    sector      = fct_lump_n(sector, 10),    # keep top 10 sectors
    corporation = as.factor(corporation),
    fiscal_year = year(ymd(rating_date)) %>% as.factor()
  ) %>%
  # Z-score scale selected continuous ratios
  mutate(across(
    c(current_ratio, debt_equity_ratio, gross_margin,
      operating_margin, net_profit_margin, roa_return_on_assets),
    ~ as.numeric(scale(.x)),
    .names = "z_{col}"
  )) %>%
  drop_na(rating_ord, sector, corporation)

# 1. Filter for rating date in 2016 and select only the needed variables
#    - binary_rating: Binary.Rating as DV
#    - rating      : Rating as DV
#    - current_ratio, long_term_debt_capital, debt_equity_ratio,
#      ebitda_margin, operating_cash_flow_per_share, sector: predictors

df_filtered <- df %>%
  filter(fiscal_year == "2016") %>%
  select(
    binary_rating,
    rating,
    current_ratio,
    long_term_debt_capital,
    debt_equity_ratio,
    ebitda_margin,
    operating_cash_flow_per_share,
    sector
  )

# 2. Create two dataframes with different target variables
#    a) df_binary: Binary.Rating as the first column
#    b) df_rating: Rating as the first column

df_binary <- df_filtered %>%
  select(
    Binary.Rating = binary_rating,
    Current.Ratio                   = current_ratio,
    Long.term.Debt...Capital        = long_term_debt_capital,
    Debt.Equity.Ratio               = debt_equity_ratio,
    EBITDA.Margin                   = ebitda_margin,
    Operating.Cash.Flow.Per.Share   = operating_cash_flow_per_share,
    Sector                          = sector
  )


df_rating <- df_filtered %>%
  select(
    Rating                          = rating,
    Current.Ratio                   = current_ratio,
    Long.term.Debt...Capital        = long_term_debt_capital,
    Debt.Equity.Ratio               = debt_equity_ratio,
    EBITDA.Margin                   = ebitda_margin,
    Operating.Cash.Flow.Per.Share   = operating_cash_flow_per_share,
    Sector                          = sector
  )
```


b. Report the number of observations, columns (with their meaning) and their data types. Indicate clearly what you will use as dependent variable/label. 

<!-- REPORT IT BELOW -->

## 2. Split the data and tranform columns as necessary. (0.5pt)
 Split the data into training (80%) and test set (80%). Transform the columns if necessary.

```{r split-data, echo=TRUE, message=FALSE}
# Load required package for splitting
# (dplyr already loaded)

set.seed(123) # for reproducibility

# Split df_binary into 80% train and 20% test
n_binary <- nrow(df_binary)
train_idx_binary <- sample(seq_len(n_binary), size = 0.8 * n_binary)
train_binary <- df_binary[train_idx_binary, ]
test_binary <- df_binary[-train_idx_binary, ]

# Ensure factor levels of Binary.Rating are consistent
train_binary$Binary.Rating <- factor(train_binary$Binary.Rating)
test_binary$Binary.Rating <- factor(test_binary$Binary.Rating, levels = levels(train_binary$Binary.Rating))

# Split df_rating into 80% train and 20% test
n_rating <- nrow(df_rating)
train_idx_rating <- sample(seq_len(n_rating), size = 0.8 * n_rating)
train_rating <- df_rating[train_idx_rating, ]
test_rating <- df_rating[-train_idx_rating, ]

# Ensure factor levels of Rating are consistent
train_rating$Rating <- factor(train_rating$Rating)
test_rating$Rating <- factor(test_rating$Rating, levels = levels(train_rating$Rating))
```

## 3. Model Exploration (3pt)

a. Fit multiple appropriate models to the dataset (as many models as there are members in the group, with a minimum of two models). Models might vary in the multilevel structure, informativeness of their priors (but not just trivial changes), model of the data/likelihood, etc. (I recommend not to use no pooling models since they tend to take a long time and it's very hard to assign good priors). **Assess if models converged: Report how the traceplots looks like, highest Rhat, number of effective samples, etc. If didn't converge, address the issues. (If you can't solve the problems, report them and continue with the assignment).**

<!-- message = FALSE, results = "hide" prevents displaying output, if you need to show something create another chunk of code -->

{r, message = FALSE, results = "hide"}
# models go here



b. Explain each model and describe its structure (what they assume about potential population-level or group-level effects), and the type of priors used. 

<!-- EXPLAIN BELOW, REFER TO EACH MODEL -->

## 4. Model checking (3pt)

a. Perform a prior sensitivity analysis for each model and modify the model if appropriate. Justify.

```



```
<!-- EXPLAIN CONCLUSIONS AND WHETHER MODELS ARE KEPT, MODIFIED  -->

b. Conduct posterior predictive checks for each model to assess how well they fit the data.
Explain what you conclude.

```{r}

```

<!-- EXPLAIN CONCLUSIONS -->

## 5. Model Comparison (1.5pt)

a. Use k-fold cross-validation to compare the models.

```{r}

```
b. Determine the best model based on predictive accuracy and justify your decision.

<!-- DECISION -->


## 6. Interpretation of Important Parameters (1.5pt)

Choose one of the best models and interpret its most important parameters.

<!-- INTERPRETATION AND CODE GOES HERE -->
 


# Contributions of each member 

-
-
-
-

# Statement of technology


# References

<!-- Complete if necessary -->

