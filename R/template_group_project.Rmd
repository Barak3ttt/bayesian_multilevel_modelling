--
title: "Group project"
subtitle: "Group Number?"
author: 
  - "FirstName LastName"
  - "FirstName LastName"
  - "FirstName LastName"
  - "FirstName LastName"
  - "FirstName LastName"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: readable
    toc: true
    toc_depth: 4
    toc_float: true
    code_download: false
---

```{r setup, include = FALSE}
options(max.print= 120,
        width = 90,
        tibble.width = 80)
knitr::opts_chunk$set(echo= TRUE,
                      cache=FALSE,
                      prompt=FALSE,
                      tidy="styler",
                      comment=NA,
                      message=FALSE,
                      warning=TRUE)

knitr::opts_knit$set(width=90)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
set.seed(42)
```

# Checklist {-}

The submission includes the following.

- [ ] RMD document where it's clear what is the code that corresponds to each question. 
- [ ] Dataset
- [ ] html/PDF document with the following
    - [ ] Numbered questions and answers with text and all the necessary code.
    - [ ] Subtitle indicates the group number 
    - [ ] Name of all group members
    - [ ] Details of specification of the work done by group members (e.g., who found the data, who did the pre-processing, who answered which questions, etc).
    - [ ] Statement of technology. Did you use any AI tools? How?


# Group project {-}

For the project, we use the following packages:

```{r, message = FALSE}
## ...
```

## 1. Dataset Selection (0.5pt)
Select a dataset with clusters such as schools, regions, or people with multiple observations per individual. (From for example, https://www.kaggle.com/) It would be a good idea to choose a smallish dataset (not too many rows, e.g., less than 1000) or subset it so that fitting the models doesn't take too long. 

a. Describe the dataset with a couple of short sentences. What was its intended use? Are there papers that reference it? Provide information on how to obtain the dataset, including its source and any necessary preprocessing steps/feature engineering.


<!-- DESCRIBE IT BELOW -->

```{r get-data-direct, echo=TRUE, message=FALSE}
# 0) Install & load httr for HTTP requests
if (!requireNamespace("httr", quietly = TRUE)) {
  install.packages("httr", repos = "https://cran.rstudio.com/")
}
library(httr)

# 1) Kaggle credentials
user <- "barakazor"
key  <- "6bfceda688e1452d09191636ad94eeeb"

# 2) Define working and data directories
wd        <- getwd()
data_dir  <- file.path(wd, "data")
input_dir <- file.path(wd, "Input_Data")
if (!dir.exists(data_dir))  dir.create(data_dir, recursive = TRUE)
if (!dir.exists(input_dir)) dir.create(input_dir, recursive = TRUE)

# 3) Download the ZIP via Kaggle REST API
dataset_slug <- "kirtandelwadia/corporate-credit-rating-with-financial-ratios"
zip_path     <- file.path(data_dir, "credit_rating.zip")
download_url <- paste0("https://www.kaggle.com/api/v1/datasets/download/", dataset_slug)

resp <- GET(
  url             = download_url,
  authenticate(user, key),
  write_disk(zip_path, overwrite = TRUE),
  config(followlocation = TRUE)
)
stop_for_status(resp)

# 4) Unzip into data_dir
unzipped <- unzip(zip_path, exdir = data_dir)
csvs     <- unzipped[grepl("\\.csv$", unzipped)]
if (length(csvs) == 0) stop("No CSV found in the downloaded ZIP.")

# 5) Read the first CSV into R
df_raw <- read.csv(csvs[1], stringsAsFactors = FALSE)

# 6) Save to Input_Data/
write.csv(
  df_raw,
  file      = file.path(input_dir, "df_raw.csv"),
  row.names = FALSE
)
saveRDS(
  df_raw,
  file = file.path(input_dir, "df_raw.rds")
)
```


```{r preprocess-2016, echo=TRUE, message=FALSE}
# Load necessary packages (install if needed)
library(readr)
library(dplyr)
library(janitor)
library(lubridate)

# Read raw data and clean column names explicitly
input_dir <- file.path(wd, "Input_Data")   # adjust as needed
df_raw <- read_csv(file.path(input_dir, "df_raw.csv"))

# Use janitor::clean_names to ensure availability
df_raw <- janitor::clean_names(df_raw)

# Define ordered rating levels
rating_levels <- c("AAA","AA+","AA","AA-","A+","A","A-",
                   "BBB+","BBB","BBB-","BB+","BB","BB-",
                   "B+","B","B-","CCC","CC","C","D")

# Create df with fiscal_year and scaled predictors
df <- df_raw %>%
  mutate(
    rating_ord  = factor(rating, levels = rating_levels, ordered = TRUE),
    sector      = forcats::fct_lump_n(sector, 10),
    corporation = as.factor(corporation),
    fiscal_year = lubridate::year(lubridate::ymd(rating_date))
  ) %>%
  mutate(across(
    c(current_ratio, debt_equity_ratio, gross_margin,
      operating_margin, net_profit_margin, roa_return_on_assets),
    ~ as.numeric(scale(.x)), .names = "z_{col}")) %>%
  tidyr::drop_na(rating_ord, sector, corporation)



# 1. Filter for rating date in 2016 and select only the needed variables
#    - binary_rating: Binary.Rating as DV
#    - rating      : Rating as DV
#    - current_ratio, long_term_debt_capital, debt_equity_ratio,
#      ebitda_margin, operating_cash_flow_per_share, sector: predictors

df_filtered <- df %>%
  filter(fiscal_year == "2016") %>%
  select(
    binary_rating,
    rating,
    current_ratio,
    long_term_debt_capital,
    debt_equity_ratio,
    ebitda_margin,
    operating_cash_flow_per_share,
    sector
  )

# 2. Create two dataframes with different target variables
#    a) df_binary: Binary.Rating as the first column
#    b) df_rating: Rating as the first column

df_binary <- df_filtered %>%
  select(
    Binary.Rating = binary_rating,
    Current.Ratio                   = current_ratio,
    Long.term.Debt...Capital        = long_term_debt_capital,
    Debt.Equity.Ratio               = debt_equity_ratio,
    EBITDA.Margin                   = ebitda_margin,
    Operating.Cash.Flow.Per.Share   = operating_cash_flow_per_share,
    Sector                          = sector
  )


df_rating <- df_filtered %>%
  select(
    Rating                          = rating,
    Current.Ratio                   = current_ratio,
    Long.term.Debt...Capital        = long_term_debt_capital,
    Debt.Equity.Ratio               = debt_equity_ratio,
    EBITDA.Margin                   = ebitda_margin,
    Operating.Cash.Flow.Per.Share   = operating_cash_flow_per_share,
    Sector                          = sector
  )
```


b. Report the number of observations, columns (with their meaning) and their data types. Indicate clearly what you will use as dependent variable/label. 

<!-- REPORT IT BELOW -->

## 2. Split the data and tranform columns as necessary. (0.5pt)
 Split the data into training (80%) and test set (80%). Transform the columns if necessary.

```{r splitdata, echo=TRUE, message=FALSE}
# Load required package for splitting
# (dplyr already loaded)

set.seed(123) # for reproducibility

# Split df_binary into 80% train and 20% test
n_binary <- nrow(df_binary)
train_idx_binary <- sample(seq_len(n_binary), size = 0.8 * n_binary)
train_binary <- df_binary[train_idx_binary, ]
test_binary <- df_binary[-train_idx_binary, ]

# Ensure factor levels of Binary.Rating are consistent
train_binary$Binary.Rating <- factor(train_binary$Binary.Rating)
test_binary$Binary.Rating <- factor(test_binary$Binary.Rating, levels = levels(train_binary$Binary.Rating))

# Split df_rating into 80% train and 20% test
n_rating <- nrow(df_rating)
train_idx_rating <- sample(seq_len(n_rating), size = 0.8 * n_rating)
train_rating <- df_rating[train_idx_rating, ]
test_rating <- df_rating[-train_idx_rating, ]

# Ensure factor levels of Rating are consistent
train_rating$Rating <- factor(train_rating$Rating)
test_rating$Rating <- factor(test_rating$Rating, levels = levels(train_rating$Rating))
```

## 3. Model Exploration (3pt)

a. Fit multiple appropriate models to the dataset (as many models as there are members in the group, with a minimum of two models). Models might vary in the multilevel structure, informativeness of their priors (but not just trivial changes), model of the data/likelihood, etc. (I recommend not to use no pooling models since they tend to take a long time and it's very hard to assign good priors). **Assess if models converged: Report how the traceplots looks like, highest Rhat, number of effective samples, etc. If didn't converge, address the issues. (If you can't solve the problems, report them and continue with the assignment).**

```{r hyperoptimization, echo=TRUE, message=FALSE}
# bayes_opt_25_rBayes.R
# Bayesian hyper‑parameter optimisation (25 evaluations) using **rBayesianOptimization**
# instead of ParBayesianOptimization to avoid the inBounds NA bug.

# 0. Install / load packages ---------------------------------------------------
required_pkgs <- c("brms", "loo", "rBayesianOptimization", "progress")
for (pkg in required_pkgs) if (!requireNamespace(pkg, quietly = TRUE)) install.packages(pkg)

library(brms)
library(loo)
library(rBayesianOptimization)   # <‑‑ different optimiser
library(progress)

# 1. Search space --------------------------------------------------------------
search_bounds <- list(
  sigma_fixed = c(0.5, 10),
  sigma_group = c(0.5, 5),
  adapt_delta = c(0.90, 0.99),
  iter        = c(1000, 2000)
)

# helper to snap iter to 1000 / 1500 / 2000
snap_iter <- function(x) {
  cuts <- c(1000, 1500, 2000)
  cuts[which.min(abs(cuts - x))]
}

# 2. Objective function --------------------------------------------------------
obj_fun <- function(sigma_fixed, sigma_group, adapt_delta, iter) {
  iter   <- snap_iter(iter)
  warmup <- iter / 2

  priors <- c(
    prior_string(sprintf("normal(0, %.4f)", sigma_fixed), class = "b"),
    prior_string(sprintf("student_t(3, 0, %.4f)", sigma_group), class = "sd")
  )

  score <- tryCatch({
    fit <- brm(
      Binary.Rating ~ Current.Ratio +
                     `Long.term.Debt...Capital` +
                     Debt.Equity.Ratio +
                     EBITDA.Margin +
                     `Operating.Cash.Flow.Per.Share` +
                     (1 | Sector),
      data    = train_binary,
      family  = bernoulli(link = "logit"),
      prior   = priors,
      iter    = iter,
      warmup  = warmup,
      control = list(adapt_delta = adapt_delta, max_treedepth = 12),
      cores   = 4,
      refresh = 0
    )
    -loo(fit, cores = 2)$estimates["looic", "Estimate"]  # maximise
  }, error = function(e) {
    message("Penalty – ", e$message)
    -1e6
  })

  list(Score = score)
}

# 3. Run Bayesian optimisation -------------------------------------------------
set.seed(123)
opt <- BayesianOptimization(
  FUN = obj_fun,
  bounds = search_bounds,
  init_points = 10,
  n_iter      = 15,
  acq = "ucb", kappa = 2.576,
  verbose = TRUE
)

print(opt$Best_Par)
```
```{r hyperparamater_saving, echo=TRUE, message=FALSE}

# 1) Build (and create) your output directory
hyper.out.dir <- file.path(wd, "Hyperparameter_Optimization")
if (!dir.exists(hyper.out.dir)) {
  dir.create(hyper.out.dir, recursive = TRUE)
}

# 2) Construct file paths
stamp <- format(Sys.time(), "%Y%m%d_%H%M%S")

csv_file <- file.path(
  hyper.out.dir,
  paste0("best_params_", stamp, ".csv")
)
rds_file <- file.path(
  hyper.out.dir,
  paste0("rBayesOpt_res_", stamp, ".rds")
)

# 3) Write the best‐params CSV
write.csv(
  opt$Best_Par,   # the best‐parameter data.frame
  file = csv_file,
  row.names = FALSE
)

# 4) Save the full optimization result as an RDS
saveRDS(
  opt,           # the full ParBayesianOptimization result object
  file = rds_file
)

```

```{r final_model_fit, echo=TRUE, message=FALSE}
# final_model_fit.R
# Fit a Bayesian multilevel logistic regression using the best hyperparameters
# (reads them directly from the uploaded CSV)

# 1. Load libraries (will error if not installed) ----------------------------
library(brms)
library(pROC)

# ---------------------------------------------------------------------------
# 2. Locate the best-params CSV ---------------------------------------------
# ---------------------------------------------------------------------------
# Assuming your working directory is stored in the variable `wd`.
# The CSV is expected in: <wd>/Hyperparameter_Optimization/

if (!exists("wd")) {
  wd <- getwd()  # fallback – uses current working directory
  message(sprintf("`wd` not found; defaulting to getwd(): %s", wd))
}

hyper.out.dir <- file.path(wd, "Hyperparameter_Optimization")
if (!dir.exists(hyper.out.dir)) {
  stop(sprintf("Directory '%s' does not exist – check `wd`.", hyper.out.dir))
}

csv_files <- list.files(hyper.out.dir, pattern = "^best_params_.*\\.csv$", full.names = TRUE)
if (length(csv_files) == 0) {
  stop("No 'best_params_*.csv' file found in ", hyper.out.dir)
}

# Pick the newest file (in case multiple exist)
csv_file <- csv_files[order(file.info(csv_files)$mtime, decreasing = TRUE)][1]
message(sprintf("Reading hyperparameters from: %s", basename(csv_file)))

best_params_df <- read.csv(csv_file, stringsAsFactors = FALSE)

# ---------------------------------------------------------------------------
# 3. Extract hyperparameters with sane fallbacks ----------------------------
# ---------------------------------------------------------------------------
get_or <- function(df, name, default) {
  if (name %in% names(df)) df[[name]][1] else default
}

sigma_fixed <- get_or(best_params_df, "sigma_fixed", 2.5)
sigma_group <- get_or(best_params_df, "sigma_group", 1.5)
adapt_delta <- get_or(best_params_df, "adapt_delta", 0.95)
iter        <- get_or(best_params_df, "iter", 1500)
warmup      <- floor(iter / 2)

# ---------------------------------------------------------------------------
# 4. Define priors -----------------------------------------------------------
# ---------------------------------------------------------------------------
priors <- c(
  prior_string(sprintf("normal(0, %.4f)", sigma_fixed), class = "b"),
  prior_string(sprintf("student_t(3, 0, %.4f)", sigma_group), class = "sd")
)

# ---------------------------------------------------------------------------
# 5. Fit model on `train_binary` ---------------------------------------------
# ---------------------------------------------------------------------------
fit <- brm(
  formula = bf(
    Binary.Rating ~ Current.Ratio +
                   `Long.term.Debt...Capital` +
                   Debt.Equity.Ratio +
                   EBITDA.Margin +
                   `Operating.Cash.Flow.Per.Share` +
                   (1 | Sector)
  ),
  data    = train_binary,
  family  = bernoulli(link = "logit"),
  prior   = priors,
  iter    = iter,
  warmup  = warmup,
  control = list(adapt_delta = adapt_delta, max_treedepth = 12),
  cores   = 4,
  refresh = 100
)

# ---------------------------------------------------------------------------
# 6. Evaluate on `test_binary` ----------------------------------------------
# ---------------------------------------------------------------------------
probs <- colMeans(posterior_linpred(fit, newdata = test_binary, transform = TRUE))

pred_class <- factor(ifelse(probs > 0.5, 1, 0), levels = c(0, 1))
true_class <- test_binary$Binary.Rating

conf_mat <- table(Predicted = pred_class, Actual = true_class)
accuracy  <- sum(diag(conf_mat)) / sum(conf_mat)
precision <- conf_mat["1", "1"] / sum(conf_mat["1", ])
recall    <- conf_mat["1", "1"] / sum(conf_mat[, "1"])
f1_score  <- 2 * precision * recall / (precision + recall)

roc_obj   <- roc(as.numeric(as.character(true_class)), probs)
auc_value <- auc(roc_obj)

# ---------------------------------------------------------------------------
# 7. Report ------------------------------------------------------------------
# ---------------------------------------------------------------------------
print(conf_mat)


# Build a small data‑frame and round only the numeric column -----------------
metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "F1 Score", "ROC AUC"),
  Value  = c(accuracy, precision, recall, f1_score, auc_value)
)
metrics$Value <- round(metrics$Value, 3)
print(metrics, row.names = FALSE)

# In case you still want individual lines in the console (use cat so they
# always show up in stdout)
cat(sprintf("\nAccuracy : %.3f\n", accuracy))
cat(sprintf("Precision: %.3f\n", precision))
cat(sprintf("Recall   : %.3f\n", recall))
cat(sprintf("F1 Score : %.3f\n", f1_score))
cat(sprintf("ROC AUC  : %.3f\n\n", auc_value))

```

```{r diagnostic, echo=TRUE, message=FALSE}
# basic model diagnostics
print(fit)            # quick coefficients & R-hat
summary(fit)          # full convergence stats
bayesplot::mcmc_trace(as.array(fit))   # trace plots if you have bayesplot installed

# posterior predictive check
pp_check(fit)

# loo object for more detail (elpd, Pareto-k, etc.)
l <- loo(fit, cores = 2)
print(l)
```


<!-- message = FALSE, results = "hide" prevents displaying output, if you need to show something create another chunk of code -->

{r, message = FALSE, results = "hide"}
# models go here



b. Explain each model and describe its structure (what they assume about potential population-level or group-level effects), and the type of priors used. 

<!-- EXPLAIN BELOW, REFER TO EACH MODEL -->

## 4. Model checking (3pt)

a. Perform a prior sensitivity analysis for each model and modify the model if appropriate. Justify.

```



```
<!-- EXPLAIN CONCLUSIONS AND WHETHER MODELS ARE KEPT, MODIFIED  -->

b. Conduct posterior predictive checks for each model to assess how well they fit the data.
Explain what you conclude.

```{r}

```

<!-- EXPLAIN CONCLUSIONS -->

## 5. Model Comparison (1.5pt)

a. Use k-fold cross-validation to compare the models.

```{r}

```
b. Determine the best model based on predictive accuracy and justify your decision.

<!-- DECISION -->


## 6. Interpretation of Important Parameters (1.5pt)

Choose one of the best models and interpret its most important parameters.

<!-- INTERPRETATION AND CODE GOES HERE -->
 


# Contributions of each member 

-
-
-
-

# Statement of technology


# References

<!-- Complete if necessary -->

