--
title: "Group project"
subtitle: "Group Number?"
author: 
  - "Jacob Klaren"
  - "FirstName LastName"
  - "FirstName LastName"
  - "FirstName LastName"
  - "FirstName LastName"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: readable
    toc: true
    toc_depth: 4
    toc_float: true
    code_download: false
---

```{r setup, include = FALSE}
options(max.print= 120,
        width = 90,
        tibble.width = 80)
knitr::opts_chunk$set(echo= TRUE,
                      cache=FALSE,
                      prompt=FALSE,
                      tidy="styler",
                      comment=NA,
                      message=FALSE,
                      warning=TRUE)

knitr::opts_knit$set(width=90)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
set.seed(42)
```

# Checklist {-}

The submission includes the following.

- [ ] RMD document where it's clear what is the code that corresponds to each question. 
- [ ] Dataset
- [ ] html/PDF document with the following
    - [ ] Numbered questions and answers with text and all the necessary code.
    - [ ] Subtitle indicates the group number 
    - [ ] Name of all group members
    - [ ] Details of specification of the work done by group members (e.g., who found the data, who did the pre-processing, who answered which questions, etc).
    - [ ] Statement of technology. Did you use any AI tools? How?


# Group project {-}

For the project, we use the following packages:

```{r, message = FALSE}
## ...
```

## 1. Dataset Selection (0.5pt)
Select a dataset with clusters such as schools, regions, or people with multiple observations per individual. (From for example, https://www.kaggle.com/) It would be a good idea to choose a smallish dataset (not too many rows, e.g., less than 1000) or subset it so that fitting the models doesn't take too long. 

a. Describe the dataset with a couple of short sentences. What was its intended use? Are there papers that reference it? Provide information on how to obtain the dataset, including its source and any necessary preprocessing steps/feature engineering.


<!-- DESCRIBE IT BELOW -->

```{r get-data-direct, echo=TRUE, message=FALSE}
# 0) Install & load httr for HTTP requests
if (!requireNamespace("httr", quietly = TRUE)) {
  install.packages("httr", repos = "https://cran.rstudio.com/")
}
library(httr)

# 1) Kaggle credentials
user <- "barakazor"
key  <- "6bfceda688e1452d09191636ad94eeeb"

# 2) Define working and data directories
wd        <- getwd()
data_dir  <- file.path(wd, "data")
input_dir <- file.path(wd, "Input_Data")
if (!dir.exists(data_dir))  dir.create(data_dir, recursive = TRUE)
if (!dir.exists(input_dir)) dir.create(input_dir, recursive = TRUE)

# 3) Download the ZIP via Kaggle REST API
dataset_slug <- "kirtandelwadia/corporate-credit-rating-with-financial-ratios"
zip_path     <- file.path(data_dir, "credit_rating.zip")
download_url <- paste0("https://www.kaggle.com/api/v1/datasets/download/", dataset_slug)

resp <- GET(
  url             = download_url,
  authenticate(user, key),
  write_disk(zip_path, overwrite = TRUE),
  config(followlocation = TRUE)
)
stop_for_status(resp)

# 4) Unzip into data_dir
unzipped <- unzip(zip_path, exdir = data_dir)
csvs     <- unzipped[grepl("\\.csv$", unzipped)]
if (length(csvs) == 0) stop("No CSV found in the downloaded ZIP.")

# 5) Read the first CSV into R
df_raw <- read.csv(csvs[1], stringsAsFactors = FALSE)

# 6) Save to Input_Data/
write.csv(
  df_raw,
  file      = file.path(input_dir, "df_raw.csv"),
  row.names = FALSE
)
saveRDS(
  df_raw,
  file = file.path(input_dir, "df_raw.rds")
)
```


```{r preprocess-2016, echo=TRUE, message=FALSE}
# Load necessary packages (install if needed)
library(readr)
library(dplyr)
library(janitor)
library(lubridate)

# Read raw data and clean column names explicitly
input_dir <- file.path(wd, "Input_Data")   # adjust as needed
df_raw <- read_csv(file.path(input_dir, "df_raw.csv"))

# Use janitor::clean_names to ensure availability
df_raw <- janitor::clean_names(df_raw)

# Define ordered rating levels
rating_levels <- c("AAA","AA+","AA","AA-","A+","A","A-",
                   "BBB+","BBB","BBB-","BB+","BB","BB-",
                   "B+","B","B-","CCC","CC","C","D")

# Create df with fiscal_year and scaled predictors
df <- df_raw %>%
  mutate(
    rating_ord  = factor(rating, levels = rating_levels, ordered = TRUE),
    sector      = forcats::fct_lump_n(sector, 10),
    corporation = as.factor(corporation),
    fiscal_year = lubridate::year(lubridate::ymd(rating_date))
  ) %>%
  mutate(across(
    c(current_ratio, debt_equity_ratio, gross_margin,
      operating_margin, net_profit_margin, roa_return_on_assets),
    ~ as.numeric(scale(.x)), .names = "z_{col}")) %>%
  tidyr::drop_na(rating_ord, sector, corporation)



# 1. Filter for rating date in 2016 and select only the needed variables
#    - binary_rating: Binary.Rating as DV
#    - rating      : Rating as DV
#    - current_ratio, long_term_debt_capital, debt_equity_ratio,
#      ebitda_margin, operating_cash_flow_per_share, sector: predictors

df_filtered <- df %>%
  filter(fiscal_year == "2016") %>%
  select(
    binary_rating,
    rating,
    current_ratio,
    long_term_debt_capital,
    debt_equity_ratio,
    ebitda_margin,
    operating_cash_flow_per_share,
    sector
  )

# 2. Create two dataframes with different target variables
#    a) df_binary: Binary.Rating as the first column
#    b) df_rating: Rating as the first column

df_binary <- df_filtered %>%
  select(
    Binary.Rating = binary_rating,
    Current.Ratio                   = current_ratio,
    Long.term.Debt...Capital        = long_term_debt_capital,
    Debt.Equity.Ratio               = debt_equity_ratio,
    EBITDA.Margin                   = ebitda_margin,
    Operating.Cash.Flow.Per.Share   = operating_cash_flow_per_share,
    Sector                          = sector
  )


df_rating <- df_filtered %>%
  select(
    Rating                          = rating,
    Current.Ratio                   = current_ratio,
    Long.term.Debt...Capital        = long_term_debt_capital,
    Debt.Equity.Ratio               = debt_equity_ratio,
    EBITDA.Margin                   = ebitda_margin,
    Operating.Cash.Flow.Per.Share   = operating_cash_flow_per_share,
    Sector                          = sector
  )
```


b. Report the number of observations, columns (with their meaning) and their data types. Indicate clearly what you will use as dependent variable/label. 

<!-- REPORT IT BELOW -->

## 2. Split the data and tranform columns as necessary. (0.5pt)
 Split the data into training (80%) and test set (80%). Transform the columns if necessary.

```{r splitdata, echo=TRUE, message=FALSE}
# Load required package for splitting
# (dplyr already loaded)

set.seed(123) # for reproducibility

# Split df_binary into 80% train and 20% test
n_binary <- nrow(df_binary)
train_idx_binary <- sample(seq_len(n_binary), size = 0.8 * n_binary)
train_binary <- df_binary[train_idx_binary, ]
test_binary <- df_binary[-train_idx_binary, ]

# Ensure factor levels of Binary.Rating are consistent
train_binary$Binary.Rating <- factor(train_binary$Binary.Rating)
test_binary$Binary.Rating <- factor(test_binary$Binary.Rating, levels = levels(train_binary$Binary.Rating))

# Split df_rating into 80% train and 20% test
n_rating <- nrow(df_rating)
train_idx_rating <- sample(seq_len(n_rating), size = 0.8 * n_rating)
train_rating <- df_rating[train_idx_rating, ]
test_rating <- df_rating[-train_idx_rating, ]

# Ensure factor levels of Rating are consistent
train_rating$Rating <- factor(train_rating$Rating)
test_rating$Rating <- factor(test_rating$Rating, levels = levels(train_rating$Rating))
```

## 3. Model Exploration (3pt)

a. Fit multiple appropriate models to the dataset (as many models as there are members in the group, with a minimum of two models). Models might vary in the multilevel structure, informativeness of their priors (but not just trivial changes), model of the data/likelihood, etc. (I recommend not to use no pooling models since they tend to take a long time and it's very hard to assign good priors). **Assess if models converged: Report how the traceplots looks like, highest Rhat, number of effective samples, etc. If didn't converge, address the issues. (If you can't solve the problems, report them and continue with the assignment).**

```{r hyperparamater, echo=TRUE, message=FALSE}
# parbayes_opt.R
# R script for Bayesian hyperparameter optimization using ParBayesianOptimization

# 0. Install and load packages
if (!requireNamespace("ParBayesianOptimization", quietly = TRUE)) {
  install.packages("ParBayesianOptimization", repos = 'https://cran.rstudio.com/')
}
if (!requireNamespace("brms", quietly = TRUE)) {
  install.packages("brms", repos = 'https://cran.rstudio.com/')
}
if (!requireNamespace("loo", quietly = TRUE)) {
  install.packages("loo", repos = 'https://cran.rstudio.com/')
}
library(ParBayesianOptimization)
library(brms)
library(loo)

# 1. Prepare training data
# Ensure 'train_binary' exists (this part is for the original bernoulli model, can be left)
if (!exists("train_binary")) {
  train_binary <- read.csv("Input_Data/df_binary.csv")
  train_binary$Binary.Rating <- factor(train_binary$Binary.Rating)
}

# --- BEGIN MODIFIED BRM MODEL CALL ---
# Define priors for the cumulative model with cloglog link and random intercepts
cloglog_intercept_priors <- c(
  prior(normal(0, 1.5), class = b),  # For all fixed effects, including interaction
  prior(student_t(3, 0, 2.5), class = sd), # For standard deviation of random intercepts
  prior(student_t(3, 0, 2.5), class = Intercept) # For cutpoints in cumulative model
)

# Ensure train_rating is available and Rating is an ordered factor
# It should be from the splitdata chunk, but we make sure Rating is ordered here.
if (!exists("train_rating")) {
    stop("train_rating dataframe not found. Ensure the 'splitdata' chunk has been run.")
}
rating_levels <- c("AAA","AA+","AA","AA-","A+","A","A-",
                   "BBB+","BBB","BBB-","BB+","BB","BB-",
                   "B+","B","B-","CCC","CC","C","D")
train_rating$Rating <- factor(train_rating$Rating, levels = rating_levels, ordered = TRUE)


# Fit a standalone brms model (cumulative with cloglog link and random intercepts only)
print("Fitting a cumulative brm model with cloglog link and random intercepts...")
fit_cumulative_cloglog_intercepts <- brm(
  formula = bf(
    Rating ~ Current.Ratio * Debt.Equity.Ratio + # Includes Current.Ratio, Debt.Equity.Ratio, and their interaction
             `Long.term.Debt...Capital` +
             EBITDA.Margin +
             `Operating.Cash.Flow.Per.Share` +
             (1 | Sector) # Random intercepts for Sector
  ),
  data    = train_rating,
  family  = cumulative(link = "cloglog"), # Cumulative likelihood with cloglog link
  prior   = cloglog_intercept_priors,
  iter    = 2000,
  warmup  = 1000,
  control = list(adapt_delta = 0.95),
  cores   = 4,
  refresh = 0,
  seed    = 123
)
print("Cumulative cloglog brm model fitting complete.")
print(summary(fit_cumulative_cloglog_intercepts))
# --- END MODIFIED BRM MODEL CALL ---

# 2. Define the objective function with error handling
# Returns a list with 'Score' to maximize (we use -LOOIC)
# obj_fun <- function(sigma_fixed, sigma_group, adapt_delta) {
#   result <- tryCatch({
#     # Build dynamic priors as strings
#     priors <- c(
#       prior_string(paste0("normal(0, ", round(sigma_fixed,4), ")"), class = "b"),
#       prior_string(paste0("student_t(3, 0, ", round(sigma_group,4), ")"), class = "sd")
#     )
# 
#     # Fit the brms model
#     fit <- brm(
#       formula = bf(
#         Binary.Rating ~ Current.Ratio +
#                         `Long.term.Debt...Capital` +
#                         Debt.Equity.Ratio +
#                         EBITDA.Margin +
#                         `Operating.Cash.Flow.Per.Share` +
#                         (1 | Sector)
#       ),
#       data    = train_binary,
#       family  = bernoulli(link = "logit"),
#       prior   = priors,
#       iter    = 1000,
#       warmup  = 500,
#       control = list(adapt_delta = adapt_delta),
#       cores   = 4,
#       refresh = 0
#     )
# 
#     # Compute LOOIC
#     looic_val <- loo(fit)$estimates["looic", "Estimate"]
#     list(Score = -looic_val)
#   }, error = function(e) {
#     message("Objective error: ", e$message)
#     return(NULL)
#   })
#   return(result)
# }

# 3. Define search bounds
# bounds <- list(
#   sigma_fixed = c(0.1, 10),
#   sigma_group = c(0.1, 5),
#   adapt_delta = c(0.8, 0.99)
# )

# 4. Run Bayesian Optimization with progress reporting
# set.seed(123)
# opt_res <- bayesOpt(
#   FUN           = obj_fun,
#   bounds        = bounds,
#   initPoints    = 5,
#   iters.n       = 15,
#   acq           = "ucb",       # Upper confidence bound
#   kappa         = 2.576,         # exploration/exploitation trade-off
#   verbose       = 2,             # textual progress
#   plotProgress  = TRUE,          # acquisition plot
#   dropNull      = TRUE           # skip failed evaluations
# )

# 5. Examine best hyperparameters
# best_params <- getBestPars(opt_res)
# print(best_params)
```


<!-- message = FALSE, results = "hide" prevents displaying output, if you need to show something create another chunk of code -->

{r, message = FALSE, results = "hide"}
# models go here



b. Explain each model and describe its structure (what they assume about potential population-level or group-level effects), and the type of priors used. 

<!-- EXPLAIN BELOW, REFER TO EACH MODEL -->
**Model 1: Cumulative Ordinal Regression with Random Intercepts and Cloglog Link**

This model, named `fit_cumulative_cloglog_intercepts` in the R code, aims to predict the ordinal credit `Rating` based on several financial predictors.

*   **Dependent Variable**: `Rating`, treated as an ordered categorical variable.
*   **Likelihood and Link Function**: The model uses a `cumulative` likelihood with a `cloglog` (complementary log-log) link function.
    *   The cumulative likelihood is appropriate for ordinal responses, modeling the probability of a rating being in a certain category or lower.
    *   The `cloglog` link is an alternative to the more common `logit` link. It's asymmetric and can be particularly useful when the probability of one extreme (either the lowest or highest category) is more prevalent or of particular interest, or when the rate of change towards one end of the probability scale is different from the other. For example, if lower ratings are rare and the probability of transitioning out of them changes slowly at first, then more rapidly, a cloglog link might be suitable.
*   **Fixed Effects Structure**: The model includes the following predictors as fixed effects:
    *   `Current.Ratio`
    *   `Debt.Equity.Ratio`
    *   An interaction term: `Current.Ratio * Debt.Equity.Ratio` (which means the model estimates main effects for both, plus their combined effect).
    *   `Long.term.Debt...Capital`
    *   `EBITDA.Margin`
    *   `Operating.Cash.Flow.Per.Share`
    These predictors are assumed to have a consistent effect on the log-cumulative odds (or cloglog-transformed cumulative probabilities) of the `Rating` across all sectors.
*   **Random Effects Structure**: `(1 | Sector)`
    *   This specifies random intercepts for each `Sector`. It assumes that the baseline propensity for a given credit rating (i.e., the cutpoints on the latent scale) varies from sector to sector.
    *   It does *not* assume that the effects of the predictors (slopes) vary by sector.
*   **Priors**:
    *   Fixed Effects (`class = b`): `normal(0, 1.5)` - This is a weakly informative prior, suggesting that the predictor coefficients are centered around 0 with a standard deviation of 1.5 on the cloglog scale. For standardized predictors, this allows for reasonably sized effects.
    *   Standard Deviation of Random Intercepts (`class = sd` for `Sector`): `student_t(3, 0, 2.5)` - A weakly informative prior for the variability of the intercepts across sectors. The Student's t-distribution with 3 degrees of freedom has heavier tails than a normal distribution, making it robust to outliers.
    *   Cutpoints/Thresholds (`class = Intercept`): `student_t(3, 0, 2.5)` - Weakly informative priors for the estimated thresholds that define the boundaries between the ordered categories of `Rating` on the latent cloglog scale.
*   **Assumptions**:
    *   Proportional odds (or proportional hazards, in the context of cloglog): The effect of each predictor is assumed to be consistent across all cutpoints/thresholds of the ordinal outcome. This is a key assumption of standard cumulative models.
    *   The random intercepts for `Sector` are normally distributed.
    *   Observations are conditionally independent given the model parameters.
*   **Estimation**: The model was fitted using Bayesian methods with `brms`, employing 2000 total iterations per chain, with 1000 iterations as warmup.

## 4. Model checking (3pt)

a. Perform a prior sensitivity analysis for each model and modify the model if appropriate. Justify.

```



```
<!-- EXPLAIN CONCLUSIONS AND WHETHER MODELS ARE KEPT, MODIFIED  -->

b. Conduct posterior predictive checks for each model to assess how well they fit the data.
Explain what you conclude.

```{r}

```

<!-- EXPLAIN CONCLUSIONS -->

c. Binary Classification Metrics from Ordinal Model

While our primary model (`fit_cumulative_cloglog_intercepts`) predicts ordinal ratings, we can derive binary classification metrics by defining a threshold to group these ratings. For this exercise, we will classify ratings as either "Investment Grade" (AAA down to BBB-) or "Speculative Grade" (BB+ down to D).

We will predict probabilities for each rating category on the test set, sum the probabilities for all investment grade categories, and then use a 0.5 probability threshold on this sum to make a binary prediction.

```{r calculate-binary-metrics, echo=TRUE, message=FALSE, warning=FALSE}
# Ensure the brms model object is available (it should be if the hyperparameter chunk ran)
if (!exists("fit_cumulative_cloglog_intercepts")) {
  stop("Model object 'fit_cumulative_cloglog_intercepts' not found. Please ensure the model fitting chunk has been run.")
}

# Ensure the test_rating dataset is available
if (!exists("test_rating")) {
  stop("Test dataset 'test_rating' not found. Please ensure the data splitting chunk has been run.")
}

# Load necessary library for metrics if not already loaded
if (!requireNamespace("caret", quietly = TRUE)) {
  install.packages("caret", repos = 'https://cran.rstudio.com/')
}
library(caret)

# Define rating levels and investment grade threshold
rating_levels <- c("AAA","AA+","AA","AA-","A+","A","A-",
                   "BBB+","BBB","BBB-","BB+","BB","BB-",
                   "B+","B","B-","CCC","CC","C","D")
investment_grade_ratings <- c("AAA","AA+","AA","AA-","A+","A","A-", "BBB+","BBB","BBB-")

# Ensure Rating in test_rating is an ordered factor with correct levels
test_rating$Rating <- factor(test_rating$Rating, levels = rating_levels, ordered = TRUE)

# 1. Predict probabilities for each category on the test set
# posterior_epred gives expected probabilities for each category
# Using a smaller number of draws for faster execution during knitting, increase for more stable estimates.
pred_probs_ordinal <- posterior_epred(fit_cumulative_cloglog_intercepts, newdata = test_rating, ndraws = 500) 

# pred_probs_ordinal is an array: (draws, observations, categories)
# We want the mean probability for each category for each observation
mean_pred_probs_ordinal <- colMeans(pred_probs_ordinal) # Now (observations, categories)
if(ncol(mean_pred_probs_ordinal) == length(rating_levels)){
    colnames(mean_pred_probs_ordinal) <- rating_levels # Assign category names
} else {
    stop("Mismatch between number of predicted categories and length of rating_levels.")
}


# 2. Calculate the total probability of being investment grade for each observation
prob_investment_grade <- rowSums(mean_pred_probs_ordinal[, intersect(colnames(mean_pred_probs_ordinal), investment_grade_ratings), drop = FALSE])

# 3. Convert to binary predictions ("Investment" vs "Speculative") using a 0.5 threshold
predicted_binary_labels <- ifelse(prob_investment_grade >= 0.5, "Investment", "Speculative")
predicted_binary_labels <- factor(predicted_binary_labels, levels = c("Investment", "Speculative"))

# 4. Determine true binary labels from test_rating
true_binary_labels <- ifelse(test_rating$Rating %in% investment_grade_ratings, "Investment", "Speculative")
true_binary_labels <- factor(true_binary_labels, levels = c("Investment", "Speculative"))

# 5. Compute and print confusion matrix and metrics
# Check if there is variability in both true and predicted labels
if (length(unique(predicted_binary_labels)) < 2 || length(unique(true_binary_labels)) < 2) {
    message("Warning: One or both factors (predicted or true labels) have fewer than 2 levels (e.g., all predictions are 'Investment').")
    message("Standard confusion matrix metrics from caret might not be meaningful or directly computable in this scenario.")
    message("Predicted distribution:")
    print(table(predicted_binary_labels))
    message("True distribution:")
    print(table(true_binary_labels))
    # Add custom calculations if needed for this edge case, e.g. accuracy if all match.
    if(length(unique(predicted_binary_labels)) == 1 && length(unique(true_binary_labels)) == 1 && unique(predicted_binary_labels) == unique(true_binary_labels)){
      cat("Accuracy: 100% (all predictions match the single true class)\n")
    } else {
      cat("Accuracy, Precision, Recall, F1 cannot be reliably calculated by caret.\n")
    }
} else {
    conf_matrix_obj <- confusionMatrix(data = predicted_binary_labels, 
                                     reference = true_binary_labels, 
                                     positive = "Investment")

    print(conf_matrix_obj$table) # Print the confusion matrix table

    accuracy <- conf_matrix_obj$overall["Accuracy"]
    precision <- conf_matrix_obj$byClass["Precision"]
    recall <- conf_matrix_obj$byClass["Recall"]
    f1_score <- conf_matrix_obj$byClass["F1"]

    cat("\nBinary Classification Metrics (Positive Class: 'Investment'):\n")
    cat("Accuracy: ", ifelse(is.na(accuracy), "NA", round(accuracy, 4)), "\n")
    cat("Precision: ", ifelse(is.na(precision), "NA", round(precision, 4)), "\n")
    cat("Recall (Sensitivity): ", ifelse(is.na(recall), "NA", round(recall, 4)), "\n")
    cat("F1-Score: ", ifelse(is.na(f1_score), "NA", round(f1_score, 4)), "\n")
}
```

**Interpretation of Metrics:**

*   **Accuracy**: Overall, how often is the classifier correct in distinguishing between Investment and Speculative grade? ( (TP+TN)/total )
*   **Precision**: When the model predicts a rating is "Investment" grade, how often is it actually Investment grade? ( TP/(TP+FP) ) - This measures the reliability of positive predictions.
*   **Recall (Sensitivity)**: Of all the companies that are actually "Investment" grade, how many did the model correctly identify? ( TP/(TP+FN) ) - This measures how well the model finds all the positive cases.
*   **F1-Score**: The harmonic mean of Precision and Recall. It provides a single score that balances both concerns, which is especially useful if there's an imbalance between the number of Investment and Speculative grade companies.

These metrics will give a quantitative assessment of how well the ordinal model performs on this derived binary task.

## 5. Model Comparison (1.5pt)

a. Use k-fold cross-validation to compare the models.

```{r}

```
b. Determine the best model based on predictive accuracy and justify your decision.

<!-- DECISION -->


## 6. Interpretation of Important Parameters (1.5pt)

Choose one of the best models and interpret its most important parameters.

<!-- INTERPRETATION AND CODE GOES HERE -->
 


# Contributions of each member 

-
-
-
-

# Statement of technology


# References

<!-- Complete if necessary -->

