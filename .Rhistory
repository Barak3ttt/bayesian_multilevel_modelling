df_10y <- df_yields_end %>% select(Year, all_of(ten_cols_match)) %>%
rename_with(~ toupper(.), .cols = -Year)
df_long <- pivot_longer(
df_10y,
cols = -Year,
names_to = "RawCode",
values_to = "Government_Yield"
)
# 5. Extract 2-letter ISO Code from RawCode
df_long <- df_long %>%
mutate(Code = substr(RawCode, 1, 2))
# 6. Prepare final yield table
#    keep only Code, Year, Government_Yield
df_final <- df_long %>%
filter(!is.na(Government_Yield), Government_Yield != 0) %>%
select(Code, Year, Government_Yield)
message("df_final rows (non-zero yields): ", nrow(df_final))
# 7. Merge with df_raw
#    filter df_raw to codes present in df_final
df_raw2 <- df_raw %>%
rename(Year = year) %>%
mutate(iso2c = toupper(iso2c)) %>%
filter(iso2c %in% df_final$Code)
# Perform merge and drop iso2c column
# Also drop any rows without a yield (though filtered above)
df_merged <- df_raw2 %>%
left_join(df_final, by = c("iso2c" = "Code", "Year" = "Year")) %>%
select(-iso2c)
message("df_merged rows after filtering zeros: ", nrow(df_merged))
# 8. Save merged dataset
write.csv(df_merged, file.path(input_dir, "df_merged_with_yield.csv"), row.names = FALSE)
saveRDS(df_merged, file.path(input_dir, "df_merged_with_yield.rds"))
# Preview
df_merged %>% head()
# -----------------------------------------
# Load libraries
library(dplyr)
library(tidyr)
library(httr)
# Setup working directories
wd <- getwd()
data_dir <- file.path(wd, "data")
input_dir <- file.path(wd, "Input_Data")
if (!dir.exists(data_dir)) dir.create(data_dir, recursive = TRUE)
if (!dir.exists(input_dir)) dir.create(input_dir, recursive = TRUE)
# 0. Download yields dataset from Kaggle
dataset_slug <- "everget/government-bonds"
zip_path <- file.path(data_dir, "government_bonds.zip")
user <- "barakazor"
key <- "6bfceda688e1452d09191636ad94eeeb"
resp <- GET(
url = paste0("https://www.kaggle.com/api/v1/datasets/download/", dataset_slug),
authenticate(user, key),
write_disk(zip_path, overwrite = TRUE),
config(followlocation = TRUE)
)
stop_for_status(resp)
unzipped_files <- unzip(zip_path, exdir = data_dir)
yield_csv_path <- grep("yields\\.csv$", unzipped_files, value = TRUE, ignore.case = TRUE)
message("Loaded yields file: ", yield_csv_path)
# Read input files
df_raw <- read.csv(file.path(input_dir, "df_raw.csv"), stringsAsFactors = FALSE)
message("df_raw rows: ", nrow(df_raw), ", columns: ", ncol(df_raw))
df_yields <- read.csv(yield_csv_path, stringsAsFactors = FALSE)
message("df_yields rows: ", nrow(df_yields), ", columns: ", ncol(df_yields))
# 1. Convert UNIX ms to POSIXct and extract Year
df_yields <- df_yields %>%
mutate(
time = as.POSIXct(time/1000, origin = "1970-01-01", tz = "UTC"),
Year = as.integer(format(time, "%Y"))
)
# 2. Select the last available date per Year (proxy for end-of-year)
df_yields_end <- df_yields %>%
group_by(Year) %>%
slice_max(time, n = 1, with_ties = FALSE) %>%
ungroup()
# 3. Identify 10-year yield columns matching df_raw iso2c codes
raw_codes <- unique(toupper(df_raw$iso2c))
ten_cols_all <- grep("10$", names(df_yields_end), value = TRUE)
ten_cols_match <- ten_cols_all[toupper(substr(ten_cols_all,1,2)) %in% raw_codes]
# 4. Subset and pivot to long format
df_10y <- df_yields_end %>% select(Year, all_of(ten_cols_match)) %>%
rename_with(~ toupper(.), .cols = -Year)
df_long <- pivot_longer(
df_10y,
cols = -Year,
names_to = "RawCode",
values_to = "Government_Yield"
)
# 5. Extract 2-letter ISO Code from RawCode
df_long <- df_long %>%
mutate(Code = substr(RawCode, 1, 2))
# 6. Prepare final yield table
#    keep only Code, Year, Government_Yield
df_final <- df_long %>%
filter(!is.na(Government_Yield), Government_Yield != 0) %>%
select(Code, Year, Government_Yield)
message("df_final rows (non-zero yields): ", nrow(df_final))
# 7. Merge with df_raw
#    filter df_raw to codes present in df_final
df_raw2 <- df_raw %>%
rename(Year = year) %>%
mutate(iso2c = toupper(iso2c)) %>%
filter(iso2c %in% df_final$Code)
# Perform merge and drop iso2c column
# Also drop any rows without a yield (though filtered above)
df_merged <- df_raw2 %>%
left_join(df_final, by = c("iso2c" = "Code", "Year" = "Year")) %>%
select(-iso2c)
message("df_merged rows after filtering zeros: ", nrow(df_merged))
# 8. Save merged dataset
write.csv(df_merged, file.path(input_dir, "df_merged_with_yield.csv"), row.names = FALSE)
saveRDS(df_merged, file.path(input_dir, "df_merged_with_yield.rds"))
# Preview
df_merged %>% head()
# -----------------------------------------
# Load libraries
library(dplyr)
library(tidyr)
library(httr)
# Setup working directories
wd <- getwd()
data_dir <- file.path(wd, "data")
input_dir <- file.path(wd, "Input_Data")
if (!dir.exists(data_dir)) dir.create(data_dir, recursive = TRUE)
if (!dir.exists(input_dir)) dir.create(input_dir, recursive = TRUE)
# 0. Download yields dataset from Kaggle
dataset_slug <- "everget/government-bonds"
zip_path <- file.path(data_dir, "government_bonds.zip")
user <- "barakazor"
key <- "6bfceda688e1452d09191636ad94eeeb"
resp <- GET(
url = paste0("https://www.kaggle.com/api/v1/datasets/download/", dataset_slug),
authenticate(user, key),
write_disk(zip_path, overwrite = TRUE),
config(followlocation = TRUE)
)
stop_for_status(resp)
unzipped_files <- unzip(zip_path, exdir = data_dir)
yield_csv_path <- grep("yields\\.csv$", unzipped_files, value = TRUE, ignore.case = TRUE)
message("Loaded yields file: ", yield_csv_path)
# 1. Read input files
df_raw <- read.csv(file.path(input_dir, "df_raw.csv"), stringsAsFactors = FALSE)
message("df_raw rows: ", nrow(df_raw), ", columns: ", ncol(df_raw))
df_yields <- read.csv(yield_csv_path, stringsAsFactors = FALSE)
message("df_yields rows: ", nrow(df_yields), ", columns: ", ncol(df_yields))
# 2. Convert UNIX ms to POSIXct and extract Year
df_yields <- df_yields %>%
mutate(
time = as.POSIXct(time/1000, origin = "1970-01-01", tz = "UTC"),
Year = as.integer(format(time, "%Y"))
)
# 3. Select last available date per Year as proxy for end-of-year
df_yields_end <- df_yields %>%
group_by(Year) %>%
slice_max(time, n = 1, with_ties = FALSE) %>%
ungroup()
message("df_yields_end rows: ", nrow(df_yields_end))
# 4. Identify 10-year yield columns matching df_raw iso2c codes
raw_codes <- unique(toupper(df_raw$iso2c))
ten_cols_all <- grep("10$", names(df_yields_end), value = TRUE)
ten_cols_match <- ten_cols_all[toupper(substr(ten_cols_all, 1, 2)) %in% raw_codes]
message("Matching 10Y columns: ", paste(ten_cols_match, collapse = ", "))
# 5. Subset and pivot to long format
df_10y <- df_yields_end %>%
select(Year, all_of(ten_cols_match)) %>%
rename_with(~ toupper(.), .cols = -Year)
df_long <- pivot_longer(
df_10y,
cols = -Year,
names_to = "RawCode",
values_to = "Government_Yield"
)
message("df_long rows: ", nrow(df_long))
# 6. Extract 2-letter ISO Code from RawCode
df_long <- df_long %>%
mutate(Code = toupper(substr(RawCode, 1, 2)))
# 7. Prepare final yield table: drop NA or zero yields
df_final <- df_long %>%
filter(!is.na(Government_Yield) & Government_Yield != 0) %>%
select(Code, Year, Government_Yield)
message("df_final rows (non-zero yields): ", nrow(df_final))
# 8. Merge with df_raw: filter raw to matching codes and drop NA yields after merge
df_raw2 <- df_raw %>%
rename(Year = year) %>%
mutate(iso2c = toupper(iso2c)) %>%
filter(iso2c %in% df_final$Code)
message("df_raw2 rows before merge: ", nrow(df_raw2))
df_merged <- df_raw2 %>%
left_join(df_final, by = c("iso2c" = "Code", "Year" = "Year")) %>%
filter(!is.na(Government_Yield)) %>%
select(-iso2c)
message("df_merged rows after dropping NA yields: ", nrow(df_merged))
# 9. Save merged dataset
write.csv(df_merged, file.path(input_dir, "df_merged_with_yield.csv"), row.names = FALSE)
saveRDS(df_merged, file.path(input_dir, "df_merged_with_yield.rds"))
# Preview
df_merged %>% head()
# -----------------------------------------
# Load libraries
library(dplyr)
library(tidyr)
library(httr)
# Setup working directories
wd        <- getwd()
data_dir  <- file.path(wd, "data")
input_dir <- file.path(wd, "Input_Data")
if (!dir.exists(data_dir))  dir.create(data_dir, recursive = TRUE)
if (!dir.exists(input_dir)) dir.create(input_dir, recursive = TRUE)
# 0. Download yields dataset from Kaggle
dataset_slug <- "everget/government-bonds"
zip_path     <- file.path(data_dir, "government_bonds.zip")
user         <- "barakazor"
key          <- "6bfceda688e1452d09191636ad94eeeb"
resp <- GET(
url             = paste0("https://www.kaggle.com/api/v1/datasets/download/", dataset_slug),
authenticate(user, key),
write_disk(zip_path, overwrite = TRUE),
config(followlocation = TRUE)
)
stop_for_status(resp)
unzipped_files  <- unzip(zip_path, exdir = data_dir)
yield_csv_path  <- grep("yields\\.csv$", unzipped_files, value = TRUE, ignore.case = TRUE)
# 1. Read input files
df_raw    <- read.csv(file.path(input_dir, "df_raw.csv"), stringsAsFactors = FALSE)
df_yields <- read.csv(yield_csv_path,           stringsAsFactors = FALSE)
# 2. Convert time & extract Year
df_yields <- df_yields %>%
mutate(
time = as.POSIXct(time/1000, origin = "1970-01-01", tz = "UTC"),
Year = as.integer(format(time, "%Y"))
)
# 3. Select one observation per year (last available date)
df_yields_end <- df_yields %>%
group_by(Year) %>%
slice_max(time, n = 1, with_ties = FALSE) %>%
ungroup()
# 4. Identify valid 10Y columns matching df_raw iso2c
raw_codes     <- unique(toupper(df_raw$iso2c))
all_10Y_cols  <- grep("10$", names(df_yields_end), value = TRUE)
match_10Y     <- all_10Y_cols[toupper(substr(all_10Y_cols,1,2)) %in% raw_codes]
# Standardize names to uppercase
match_10Y      <- toupper(match_10Y)
# 5. Pivot to long format
df_10y <- df_yields_end %>%
select(Year, all_of(match_10Y))
colnames(df_10y)[-1] <- match_10Y
df_long <- pivot_longer(
df_10y,
cols      = -Year,
names_to  = "Code",
values_to = "Government_Yield"
)
# 6. Filter out missing or zero yields
df_long <- df_long %>%
filter(!is.na(Government_Yield), Government_Yield != 0)
# 7. Restrict to codes with full data 2002â€“2019
years_needed <- 2002:2019
valid_codes  <- df_long %>%
filter(Year %in% years_needed) %>%
group_by(Code) %>%
summarise(n_years = n_distinct(Year), .groups = "drop") %>%
filter(n_years == length(years_needed)) %>%
pull(Code)
df_final <- df_long %>%
filter(Code %in% valid_codes)
# 8. Merge with df_raw (keep only matching countries)
df_merged <- df_raw %>%
rename(Year = year) %>%
mutate(iso2c = toupper(iso2c)) %>%
filter(iso2c %in% valid_codes) %>%
left_join(
df_final %>% mutate(Code = toupper(Code)),
by = c("iso2c" = "Code", "Year")
) %>%
select(-iso2c)  # drop duplicate code column
# 9. Save merged dataset
write.csv(df_merged, file.path(input_dir, "df_merged_with_yield.csv"), row.names = FALSE)
saveRDS(df_merged,   file.path(input_dir, "df_merged_with_yield.rds"))
# Preview
head(df_merged)
# -----------------------------------------
# Load libraries
library(dplyr)
library(tidyr)
library(httr)
# Setup working directories
wd <- getwd()
data_dir <- file.path(wd, "data")
input_dir <- file.path(wd, "Input_Data")
if (!dir.exists(data_dir)) dir.create(data_dir, recursive = TRUE)
if (!dir.exists(input_dir)) dir.create(input_dir, recursive = TRUE)
# 0. Download yields dataset from Kaggle
dataset_slug <- "everget/government-bonds"
zip_path <- file.path(data_dir, "government_bonds.zip")
user <- "barakazor"
key <- "6bfceda688e1452d09191636ad94eeeb"
resp <- GET(
url = paste0("https://www.kaggle.com/api/v1/datasets/download/", dataset_slug),
authenticate(user, key),
write_disk(zip_path, overwrite = TRUE),
config(followlocation = TRUE)
)
stop_for_status(resp)
unzipped_files <- unzip(zip_path, exdir = data_dir)
yield_csv_path <- grep("yields\\.csv$", unzipped_files, value = TRUE, ignore.case = TRUE)
message("Loaded yields file: ", yield_csv_path)
# 1. Read input files
df_raw <- read.csv(file.path(input_dir, "df_raw.csv"), stringsAsFactors = FALSE)
message("df_raw rows: ", nrow(df_raw), ", columns: ", ncol(df_raw))
df_yields <- read.csv(yield_csv_path, stringsAsFactors = FALSE)
message("df_yields rows: ", nrow(df_yields), ", columns: ", ncol(df_yields))
# 2. Convert UNIX ms to POSIXct and extract Year
df_yields <- df_yields %>%
mutate(
time = as.POSIXct(time/1000, origin = "1970-01-01", tz = "UTC"),
Year = as.integer(format(time, "%Y"))
)
# 3. Select last available date per Year as proxy for end-of-year
df_yields_end <- df_yields %>%
group_by(Year) %>%
slice_max(time, n = 1, with_ties = FALSE) %>%
ungroup()
message("df_yields_end rows: ", nrow(df_yields_end))
# 4. Identify 10-year yield columns matching df_raw iso2c codes
raw_codes <- unique(toupper(df_raw$iso2c))
ten_cols_all <- grep("10$", names(df_yields_end), value = TRUE)
ten_cols_match <- ten_cols_all[toupper(substr(ten_cols_all, 1, 2)) %in% raw_codes]
message("Matching 10Y columns: ", paste(ten_cols_match, collapse = ", "))
# 5. Subset and pivot to long format
df_10y <- df_yields_end %>%
select(Year, all_of(ten_cols_match)) %>%
rename_with(~ toupper(.), .cols = -Year)
df_long <- pivot_longer(
df_10y,
cols = -Year,
names_to = "RawCode",
values_to = "Government_Yield"
)
message("df_long rows: ", nrow(df_long))
# 6. Extract 2-letter ISO Code from RawCode
df_long <- df_long %>%
mutate(Code = toupper(substr(RawCode, 1, 2)))
# 7. Prepare final yield table: drop NA or zero yields
df_final <- df_long %>%
filter(!is.na(Government_Yield) & Government_Yield != 0) %>%
select(Code, Year, Government_Yield)
message("df_final rows (non-zero yields): ", nrow(df_final))
# 8. Merge with df_raw: filter raw to matching codes and drop NA yields after merge
df_raw2 <- df_raw %>%
rename(Year = year) %>%
mutate(iso2c = toupper(iso2c)) %>%
filter(iso2c %in% df_final$Code)
message("df_raw2 rows before merge: ", nrow(df_raw2))
df_merged <- df_raw2 %>%
left_join(df_final, by = c("iso2c" = "Code", "Year" = "Year")) %>%
filter(!is.na(Government_Yield)) %>%
select(-iso2c)
message("df_merged rows after dropping NA yields: ", nrow(df_merged))
# 9. Save merged dataset
write.csv(df_merged, file.path(input_dir, "df_merged_with_yield.csv"), row.names = FALSE)
saveRDS(df_merged, file.path(input_dir, "df_merged_with_yield.rds"))
# Preview
df_merged %>% head()
# -----------------------------------------
# Load libraries
library(dplyr)
library(tidyr)
library(httr)
# Setup working directories
wd <- getwd()
data_dir <- file.path(wd, "data")
input_dir <- file.path(wd, "Input_Data")
if (!dir.exists(data_dir)) dir.create(data_dir, recursive = TRUE)
if (!dir.exists(input_dir)) dir.create(input_dir, recursive = TRUE)
# 0. Download yields dataset from Kaggle
dataset_slug <- "everget/government-bonds"
zip_path <- file.path(data_dir, "government_bonds.zip")
user <- "barakazor"
key <- "6bfceda688e1452d09191636ad94eeeb"
resp <- GET(
url = paste0("https://www.kaggle.com/api/v1/datasets/download/", dataset_slug),
authenticate(user, key),
write_disk(zip_path, overwrite = TRUE),
config(followlocation = TRUE)
)
stop_for_status(resp)
unzipped_files <- unzip(zip_path, exdir = data_dir)
yield_csv_path <- grep("yields\\.csv$", unzipped_files, value = TRUE, ignore.case = TRUE)
message("Loaded yields file: ", yield_csv_path)
# 1. Read input files
df_raw <- read.csv(file.path(input_dir, "df_raw.csv"), stringsAsFactors = FALSE)
message("df_raw rows: ", nrow(df_raw), ", columns: ", ncol(df_raw))
df_yields <- read.csv(yield_csv_path, stringsAsFactors = FALSE)
message("df_yields rows: ", nrow(df_yields), ", columns: ", ncol(df_yields))
# 2. Convert UNIX ms to POSIXct and extract Year
df_yields <- df_yields %>%
mutate(
time = as.POSIXct(time/1000, origin = "1970-01-01", tz = "UTC"),
Year = as.integer(format(time, "%Y"))
)
# 3. Select last available date per Year as proxy for end-of-year
df_yields_end <- df_yields %>%
group_by(Year) %>%
slice_max(time, n = 1, with_ties = FALSE) %>%
ungroup()
message("df_yields_end rows: ", nrow(df_yields_end))
# 4. Identify 10-year yield columns matching df_raw iso2c codes
raw_codes <- unique(toupper(df_raw$iso2c))
ten_cols_all <- grep("10$", names(df_yields_end), value = TRUE)
ten_cols_match <- ten_cols_all[toupper(substr(ten_cols_all, 1, 2)) %in% raw_codes]
message("Matching 10Y columns: ", paste(ten_cols_match, collapse = ", "))
# 5. Subset and pivot to long format
df_10y <- df_yields_end %>%
select(Year, all_of(ten_cols_match)) %>%
rename_with(~ toupper(.), .cols = -Year)
df_long <- pivot_longer(
df_10y,
cols = -Year,
names_to = "RawCode",
values_to = "Government_Yield"
)
message("df_long rows: ", nrow(df_long))
# 6. Extract 2-letter ISO Code from RawCode
df_long <- df_long %>%
mutate(Code = toupper(substr(RawCode, 1, 2)))
# 7. Prepare final yield table: drop NA or zero yields
df_final <- df_long %>%
filter(!is.na(Government_Yield) & Government_Yield != 0) %>%
select(Code, Year, Government_Yield)
message("df_final rows (non-zero yields): ", nrow(df_final))
# 8. Merge with df_raw: filter raw to matching codes and drop NA yields after merge
df_raw2 <- df_raw %>%
rename(Year = year) %>%
mutate(iso2c = toupper(iso2c)) %>%
filter(iso2c %in% df_final$Code)
message("df_raw2 rows before merge: ", nrow(df_raw2))
df_merged <- df_raw2 %>%
left_join(df_final, by = c("iso2c" = "Code", "Year" = "Year")) %>%
filter(!is.na(Government_Yield)) %>%
select(-iso2c)
message("df_merged rows after dropping NA yields: ", nrow(df_merged))
# 9. Save merged dataset
write.csv(df_merged, file.path(input_dir, "df_merged_with_yield.csv"), row.names = FALSE)
saveRDS(df_merged, file.path(input_dir, "df_merged_with_yield.rds"))
# a) Keep only observations in the 2002â€“2019 window
df_merged_windowed <- df_merged %>%
filter(Year >= 2002, Year <= 2019)
# b) Identify countries with a complete panel (one obs per year for all 18 years)
#    Here I assume df_merged has a column `country` (or replace with your iso3c/iso2c field)
complete_countries <- df_merged_windowed %>%
group_by(country) %>%
summarize(n_years = n_distinct(Year), .groups = "drop") %>%
filter(n_years == (2019 - 2002 + 1)) %>%
pull(country)
# c) Subset to only those complete countries
df_final <- df_merged_windowed %>%
filter(country %in% complete_countries)
# d) Save
write.csv(df_final, file.path(input_dir, "df_final_2002_2019_complete.csv"), row.names = FALSE)
saveRDS(df_final, file.path(input_dir, "df_final_2002_2019_complete.rds"))
# Stage everything
system("git add .")
# Commit with a message
system("git commit -m \"Preprocessing that merges 2 datasets\"")
# Push to the remote (assumes origin + branch already configured)
system("git push origin main")
# 2. Initialize Git (if you haven't already):
system("git init")
# 3. (Re)name your branch to 'main':
system("git branch -M main")
# 4. Add the GitHub remote:
system("git remote add origin https://github.com/Barak3ttt/bayesian_multilevel_modelling.git")
# 5. Stage all changes:
system("git add .")
getcwd()
cwd()
wd()
getwd()
list.dirs(path = getwd(), full.names = TRUE, recursive = FALSE)
# 1. Point R at your project folder
setwd("C:/Users/azorb/PycharmProjects/bmmProject")
getwd()  # should return "C:/Users/azorb/PycharmProjects/bmmProject"
# 2. Check whatâ€™s changed
system("git status")
# 3. Stage all changes
system("git add -A")
# 4. Commit with a descriptive message (edit the text in quotes)
system("git commit -m \"Describe what you changed here\"")
# 5. Push your commit to GitHub on the main branch
system("git push origin main")
# 1. Point R at your project folder
setwd("C:/Users/azorb/PycharmProjects/bmmProject")
getwd()  # should return "C:/Users/azorb/PycharmProjects/bmmProject"
# 2. Check whatâ€™s changed
system("git status")
# 3. Stage all changes
system("git add -A")
# 4. Commit with a descriptive message (edit the text in quotes)
system("git commit -m \"Preprocessed with 2 Datasets Merged\"")
# 5. Push your commit to GitHub on the main branch
system("git push origin main")
# 1. Tell Git who you are (only once per machine is needed)
system('git config --global user.email "azorbarak@gmail.com"')
system('git config --global user.name  "Barak Azorb"')
# 2. (Optional, but recommended) Cache your credentials so youâ€™re not prompted every time:
#    On Windows this uses the built-in manager; on macOS it uses Keychain.
system('git config --global credential.helper manager-core')
# 3. Commit your staged changes
system('git commit -m "Preprocessed with 2 Datasets Merged"')
# 4. Push to GitHub (you will be prompted)
system('git push origin main')
